'''
(1) LinearSVC는 데이터가 선형적으로 구분될 때 좋은 성능을 보이며, 
속도가 빠르고 일반적으로 다른 분류 알고리즘보다 높은 정확도를 가지는 것으로 알려져 있음
    이진 분류때 사용, VC 클래스보다 학습 속도가 빠르며, 대용량 데이터셋에서 더욱 효율적.

(2) LogisticRegression: 이진 분류 문제에서 사용되는 모델로, 
선형 회귀 기법을 사용하여 로지스틱 함수를 적용한 모델
이 모델은 예측된 결과가 0 또는 1과 같은 이진 값으로 나타나며, 확률값으로 해석될 수 있음 
즉, 클래스 레이블을 예측하는 데 사용됨

(3) DecisionTreeRegressor: 회귀 분석 문제에서 사용되는 모델로, 결정 트리를 기반으로 함
이 모델은 입력 변수를 분할하여 예측 값을 계산하며, 각 단계에서 가장 좋은 분할을 선택하여 트리를 구성함 
이 모델은 특성 중요도를 제공하며, 트리의 깊이를 조절하여 모델을 정규화할 수 있음

(4) DecisionTreeClassifier: 분류 문제에서 사용되는 모델로, DecisionTreeRegressor와 마찬가지로 결정 트리를 기반으로 함
이 모델은 클래스 레이블을 예측하기 위해 입력 변수를 분할하여 트리를 구성함 
각 단계에서 가장 좋은 분할을 선택함 트리의 깊이를 조절하여 모델을 정규화할 수 있음

(5) RandomForestRegressor: 회귀 분석 문제에서 사용되는 모델로, 여러 개의 결정 트리를 기반으로 함
이 모델은 각 결정 트리에서 계산된 예측 값을 평균하여 최종 예측 값을 계산
이 모델은 과적합을 줄이는 데 효과적이며, 특성 중요도를 제공.

=> ogisticRegression은 이진 분류 문제를 해결하는 선형 모델이며, 
DecisionTreeRegressor와 DecisionTreeClassifier는 결정 트리를 기반으로 하는 회귀 분석 및 분류 문제를 해결하는 모델
RandomForestRegressor는 여러 개의 결정 트리를 기반으로 하는 회귀 분석 모델

#####

(1) Regressor는 연속형 데이터를 예측하는 모델
예를 들어, 집의 크기나 가격과 같은 연속적인 값을 예측하는 문제를 다룸

(2) Classifier는 이산형 데이터를 예측하는 모델 
예를 들어, 사진에서 고양이와 개를 구분하는 문제와 같이 몇 가지 이산적인 클래스 중 하나를 예측하는 문제를 다룸
따라서, Regressor는 출력 값이 연속적이고 실수 값이며, 
예측 값이 주로 수치적으로 계산되는 반면, Classifier는 출력 값이 이산적이며, 예측 값이 클래스 레이블과 같은 이산적인 값임

#####

(1) 퍼셉트론(Perceptron)은 이진 분류 문제에 대한 선형 분류기 중 하나임
하나 이상의 입력값으로부터 가중합을 구하고, 이를 임계치와 비교하여 출력값을 결정함
이 때, 가중치는 학습 알고리즘을 통해 업데이트됨.
퍼셉트론은 초기 인공신경망의 일종으로, 1957년에 Frank Rosenblatt이 발표함
퍼셉트론 학습 알고리즘은 단순하고 직관적이며, 적은 수의 데이터셋에서도 효과적으로 작동함

(2) cross_val_score는 교차 검증(Cross Validation)을 수행하여 모델의 성능을 평가하는 함수 
교차 검증은 학습 데이터셋을 여러 개의 폴드(Fold)로 나누어 각 폴드를 한번씩 테스트 데이터셋으로 사용하는 방법

(3) StratifiedKFold는 K-Fold와 마찬가지로 데이터셋을 K개의 폴드로 나누어 K번 모델을 학습하고 검증하는 방법 
다만, StratifiedKFold는 각 폴드 안의 데이터셋이 원래 데이터셋의 클래스 비율과 유사하도록 분할함 
이 방법은 불균형한 데이터셋에서도 모델의 검증 결과가 더욱 신뢰할 수 있도록 보장함
StratifiedKFold는 scikit-learn의 model_selection 모듈에 포함되어 있으며,
사용 방법은 KFold와 동일함 다만, KFold와 달리 StratifiedKFold는 y값(타깃 변수)을 필요로함
이 값은 데이터셋의 클래스를 나타내며, 
각 클래스별 비율을 유지하는 방식으로 데이터셋이 분할됨

'''